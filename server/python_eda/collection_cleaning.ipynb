{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def convert_to_adage_json(df, dataset_id):\n",
    "    adage_data_model = {\n",
    "        \"data_source\": \"Australian Financial Review\",\n",
    "        \"dataset_type\": \"News_Articles\",\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"time_object\": {\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"timezone\": \"GMT+11\"\n",
    "        },\n",
    "        \"events\": []\n",
    "    }\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        event = {\n",
    "            \"time_object\": {\n",
    "                \"timestamp\": row[\"modified\"].isoformat(),\n",
    "                \"duration\": 0,\n",
    "                \"duration_unit\": \"second\",\n",
    "                \"timezone\": \"GMT+11\"\n",
    "            },\n",
    "            \"event_type\": \"article\",\n",
    "            \"attribute\": {\n",
    "                \"guid\": row[\"guid\"],\n",
    "                \"byline\": row[\"byline\"],\n",
    "                \"headline\": row[\"headline\"],\n",
    "                \"section\": row[\"section\"],\n",
    "                \"publication_date\": row[\"publication_date\"].strftime(\"%Y-%m-%d\"),\n",
    "                \"page_no\": row[\"page_no\"],\n",
    "                \"classifications\": row[\"classifications\"],\n",
    "                \"text\": row.get(\"text\")\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        adage_data_model[\"events\"].append(event)\n",
    "    \n",
    "    adage_data_model[\"time_object\"][\"timestamp\"] = df[\"modified\"].max().isoformat()\n",
    "    \n",
    "    return json.dumps(adage_data_model, indent=4)\n",
    "\n",
    "def process_xml_file(file_path):\n",
    "    xml_data = open(file_path).read()\n",
    "    parser = etree.XMLParser(ns_clean=True)\n",
    "    xml = etree.parse(io.StringIO(xml_data), parser)\n",
    "    data = []\n",
    "\n",
    "    for dossier in xml.xpath('//dcdossier'):\n",
    "        guid = dossier.get('guid')\n",
    "        modified = dossier.get('modified')\n",
    "        \n",
    "        for doc in dossier.xpath('.//document'):\n",
    "            newspaper_code = doc.xpath('.//NEWSPAPERCODE/text()')\n",
    "            section = doc.xpath('.//SECTION/text()')\n",
    "            story_name = doc.xpath('.//STORYNAME/text()')\n",
    "            publication_date = doc.xpath('.//PUBLICATIONDATE/text()')\n",
    "            newspaper = doc.xpath('.//NEWSPAPER/text()')\n",
    "            page_no = doc.xpath('.//PAGENO/text()')\n",
    "            byline = doc.xpath('.//BYLINE/text()')\n",
    "            classifications = doc.xpath('.//CLASSIFICATION/text()')\n",
    "            headline = doc.xpath('.//HEADLINE/text()')\n",
    "            intro = doc.xpath('.//INTRO/text()')\n",
    "            text = \" \".join(doc.xpath('.//TEXT//text()'))\n",
    "            \n",
    "            data.append({\n",
    "                'guid': guid,\n",
    "                'modified': pd.to_datetime(modified, errors='coerce', utc=True),\n",
    "                'section': section[0].strip() if section else None,\n",
    "                'publication_date': pd.to_datetime(publication_date[0]) if publication_date else None,\n",
    "                'page_no': page_no[0].strip() if page_no else None,\n",
    "                'byline': byline[0].strip() if byline else None,\n",
    "                'classifications': classifications if classifications else None,\n",
    "                'headline': headline[0].strip() if headline else None,\n",
    "                'intro': intro[0].strip() if intro else None,\n",
    "                'text': text.strip() if text else None,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def process_all_files(directory):\n",
    "    all_dataframes = []\n",
    "    j = 0\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.xml'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = process_xml_file(file_path)\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"Processed data from {file_name}\")\n",
    "            j += 1\n",
    "        if j == 10: \n",
    "            break\n",
    "\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data from AFR_20150101-20150131.xml\n",
      "Processed data from AFR_20150201-20150201.xml\n",
      "Processed data from AFR_20150201-20150228.xml\n",
      "Processed data from AFR_20150301-20150331.xml\n",
      "Processed data from AFR_20150401-20150430.xml\n",
      "Processed data from AFR_20150501-20150531.xml\n",
      "Processed data from AFR_20150601-20150630.xml\n",
      "Processed data from AFR_20150701-20150731.xml\n",
      "Processed data from AFR_20150801-20150831.xml\n",
      "Processed data from AFR_20150901-20150930.xml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = process_all_files('datasets/')\n",
    "# df = pd.DataFrame(data)\n",
    "df['modified'] = pd.to_datetime(df['modified'])\n",
    "df['publication_date'] = pd.to_datetime(df['publication_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_prepare_text(input_text, pattern=r'[^\\w\\s]', to_lowercase=True, filter_stopwords=True, stemming=True):\n",
    "    cleaned_text = re.sub(pattern, '', input_text)\n",
    "    if to_lowercase:\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "    tokenized_text = word_tokenize(cleaned_text)\n",
    "    if filter_stopwords:\n",
    "        tokenized_text = [word for word in tokenized_text if word not in nltk_stopwords]\n",
    "    if stemming:\n",
    "        processor = PorterStemmer()\n",
    "    else:\n",
    "        processor = WordNetLemmatizer()\n",
    "    processed_text = [processor.stem(word) if stemming else processor.lemmatize(word) for word in tokenized_text]\n",
    "    return ' '.join(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_settings = (r'[^\\w\\s]', False, True, False)\n",
    "pattern, lower, stopword_removal, stem = preprocess_settings\n",
    "preprocess_fn = lambda x: clean_and_prepare_text(x, pattern, lower, stopword_removal, stem)\n",
    "df['pre_processed_text'] = df['text'].apply(preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A highprofile inquest death teenager Alec Meikle allegedly bullied Downer EDI ended coroner deciding wasnt enough evidence blame bullying suicide The Coroners Court Sydney told teenager Mr Meikle taunted workmate supervisor three month company trainbuilding operation Bathurst Mr Meikle quit moved New Zealand live aunt uncle He committed suicide 40 day arrival The death triggered debate whether workplace bullying responsible suicide NSW Deputy State Coroner Paul McMahon said could make finding recommendation death lack evidence caused NSW Mr Meikles family said deeply disappointed ruling We concerned would take 13 month come conclusion finding We hope medium attention surrounding matter evidence heard publicly lead greater awareness possible devastating consequence harassment family said Mr McMahon said uncontroversial Mr Meikles job significant contributing factor led development depressive condition factor It would speculation find event Downer precipitated action It could well something else said I therefore satisfied standard proof required cause death occurred NSWEgregious incident The egregious incident Alec suffered coroner said sphincter dilation chart created colleague David Hall Ben Eagle endorsed supervisor Colin Wiggins The chart marked Mr Meikles error threatened sexual assault reached top Mr Meikle diagnosed severe anxiety depression spent short period Bathurst Hospitals mental health unit quit His parent arranged move back New Zealand attempted suicide following discharge hospital He moved Australia New Zealand parent 2002 A Downer EDI spokesman Michael Sharp said death tragedy excuse behaviour occurred The company strengthened policy procedure relation bullying harassment discrimination past five year continually reviewing Mr Sharp said We dont tolerate everyone Downer right work workplace free bullying harassment discrimination said Key point Alec Meikle endured three month taunting bos workmate working Bathurst NSW The teenager died suicide New Zealand'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pre_processed_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def get_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    return blob.sentiment.polarity, blob.sentiment.subjectivity\n",
    "\n",
    "df[['sentiment_polarity', 'sentiment_subjectivity']] = df['text'].apply(lambda x: pd.Series(get_sentiment(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "dictionary = corpora.Dictionary([text.split() for text in df['pre_processed_text']])\n",
    "corpus = [dictionary.doc2bow(text.split()) for text in df['pre_processed_text']]\n",
    "\n",
    "lda_model = models.LdaMulticore(corpus, num_topics=5, id2word=dictionary, passes=10)\n",
    "\n",
    "def get_dominant_topic(text):\n",
    "    bow = dictionary.doc2bow(text.split())\n",
    "    topics = lda_model.get_document_topics(bow)\n",
    "    dominant_topic = max(topics, key=lambda x: x[1])[0]\n",
    "    topic_keywords = ', '.join([word for word, _ in lda_model.show_topic(dominant_topic)])\n",
    "    return topic_keywords\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dominant_topic'] = df['pre_processed_text'].apply(get_dominant_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_sentence_length(text):\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) == 0:\n",
    "        return 0  # Return 0 if there are no sentences\n",
    "    return sum(len(nltk.word_tokenize(sentence)) for sentence in sentences) / len(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['avg_sentence_length'] = df['text'].apply(avg_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "def extract_keywords(text):\n",
    "    rake = Rake()\n",
    "    rake.extract_keywords_from_text(text)\n",
    "    return rake.get_ranked_phrases()[:5]\n",
    "\n",
    "df['keywords'] = df['pre_processed_text'].apply(extract_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seng",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
