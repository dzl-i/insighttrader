{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data from AFR_20150901-20150930.xml\n",
      "Processed data from AFR_20201001-20201031.xml\n",
      "Processed data from AFR_20180801-20180831.xml\n",
      "Processed data from AFR_20190901-20190930.xml\n",
      "Processed data from AFR_20211101-20211130.xml\n",
      "Processed data from AFR_20210201-20210228.xml\n",
      "Processed data from AFR_20211201-20211231.xml\n",
      "Processed data from AFR_20210901-20210930.xml\n",
      "Processed data from AFR_20191101-20191130.xml\n",
      "Processed data from AFR_20160501-20160531.xml\n",
      "Processed data from AFR_20160301-20160331.xml\n",
      "Processed data from AFR_20190201-20190228.xml\n",
      "Processed data from AFR_20191201-20191231.xml\n",
      "Processed data from AFR_20160601-20160630.xml\n",
      "Processed data from AFR_20150201-20150201.xml\n",
      "Processed data from AFR_20170701-20170731.xml\n",
      "Processed data from AFR_20151101-20151130.xml\n",
      "Processed data from AFR_20181001-20181031.xml\n",
      "Processed data from AFR_20170401-20170430.xml\n",
      "Processed data from AFR_20200801-20200831.xml\n",
      "Processed data from AFR_20170101-20170131.xml\n",
      "Processed data from AFR_20151201-20151231.xml\n",
      "Processed data from AFR_20150201-20150228.xml\n",
      "Processed data from AFR_20210801-20210831.xml\n",
      "Processed data from AFR_20160101-20160131.xml\n",
      "Processed data from AFR_20191001-20191031.xml\n",
      "Processed data from AFR_20160401-20160430.xml\n",
      "Processed data from AFR_20160701-20160731.xml\n",
      "Processed data from AFR_20181201-20181231.xml\n",
      "Processed data from AFR_20180201-20180228.xml\n",
      "Processed data from AFR_20170601-20170630.xml\n",
      "Processed data from AFR_20151001-20151031.xml\n",
      "Processed data from AFR_20170301-20170331.xml\n",
      "Processed data from AFR_20181101-20181130.xml\n",
      "Processed data from AFR_20170501-20170531.xml\n",
      "Processed data from AFR_20200901-20200930.xml\n",
      "Processed data from AFR_20150801-20150831.xml\n",
      "Processed data from AFR_20201201-20201231.xml\n",
      "Processed data from AFR_20201101-20201130.xml\n",
      "Processed data from AFR_20200201-20200229.xml\n",
      "Processed data from AFR_20180901-20180930.xml\n",
      "Processed data from AFR_20190801-20190831.xml\n",
      "Processed data from AFR_20211001-20211031.xml\n",
      "Processed data from AFR_20210501-20210531.xml\n",
      "Processed data from AFR_20160901-20160930.xml\n",
      "Processed data from AFR_20210601-20210630.xml\n",
      "Processed data from AFR_20210301-20210331.xml\n",
      "Processed data from AFR_20200701-20200731.xml\n",
      "Processed data from AFR_20170801-20170831.xml\n",
      "Processed data from AFR_20200101-20200131.xml\n",
      "Processed data from AFR_20200401-20200430.xml\n",
      "Processed data from AFR_20150501-20150531.xml\n",
      "Processed data from AFR_20180701-20180731.xml\n",
      "Processed data from AFR_20180101-20180131.xml\n",
      "Processed data from AFR_20150601-20150630.xml\n",
      "Processed data from AFR_20180401-20180430.xml\n",
      "Processed data from AFR_20171001-20171031.xml\n",
      "Processed data from AFR_20150301-20150331.xml\n",
      "Processed data from AFR_20190501-20190531.xml\n",
      "Processed data from AFR_20161101-20161130.xml\n",
      "Processed data from AFR_20160201-20160229.xml\n",
      "Processed data from AFR_20190601-20190630.xml\n",
      "Processed data from AFR_20161201-20161231.xml\n",
      "Processed data from AFR_20190301-20190331.xml\n",
      "Processed data from AFR_20180301-20180331.xml\n",
      "Processed data from AFR_20150401-20150430.xml\n",
      "Processed data from AFR_20180601-20180630.xml\n",
      "Processed data from AFR_20150101-20150131.xml\n",
      "Processed data from AFR_20171201-20171231.xml\n",
      "Processed data from AFR_20170201-20170228.xml\n",
      "Processed data from AFR_20150701-20150731.xml\n",
      "Processed data from AFR_20180501-20180531.xml\n",
      "Processed data from AFR_20171101-20171130.xml\n",
      "Processed data from AFR_20190401-20190430.xml\n",
      "Processed data from AFR_20161001-20161031.xml\n",
      "Processed data from AFR_20190101-20190131.xml\n",
      "Processed data from AFR_20190701-20190731.xml\n",
      "Processed data from AFR_20210401-20210430.xml\n",
      "Processed data from AFR_20160801-20160831.xml\n",
      "Processed data from AFR_20210101-20210131.xml\n",
      "Processed data from AFR_20210701-20210731.xml\n",
      "Processed data from AFR_20200301-20200331.xml\n",
      "Processed data from AFR_20200601-20200630.xml\n",
      "Processed data from AFR_20170901-20170930.xml\n",
      "Processed data from AFR_20200501-20200531.xml\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import io\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, jaccard_score\n",
    "import re\n",
    "import os\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "mlb = MultiLabelBinarizer()\n",
    "multi_label_nb = MultiOutputClassifier(MultinomialNB(), n_jobs=-1)\n",
    "\n",
    "def convert_to_adage_json(df, dataset_id):\n",
    "    adage_data_model = {\n",
    "        \"data_source\": \"Australian Financial Review\",\n",
    "        \"dataset_type\": \"News_Articles\",\n",
    "        \"dataset_id\": dataset_id,\n",
    "        \"time_object\": {\n",
    "            \"timestamp\": pd.Timestamp.now().isoformat(),\n",
    "            \"timezone\": \"GMT+11\"\n",
    "        },\n",
    "        \"events\": []\n",
    "    }\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        event = {\n",
    "            \"time_object\": {\n",
    "                \"timestamp\": row[\"modified\"].isoformat(),\n",
    "                \"duration\": 0,\n",
    "                \"duration_unit\": \"second\",\n",
    "                \"timezone\": \"GMT+11\"\n",
    "            },\n",
    "            \"event_type\": \"article\",\n",
    "            \"attribute\": {\n",
    "                \"guid\": row[\"guid\"],\n",
    "                \"byline\": row[\"byline\"],\n",
    "                \"headline\": row[\"headline\"],\n",
    "                \"section\": row[\"section\"],\n",
    "                \"publication_date\": row[\"publication_date\"].strftime(\"%Y-%m-%d\"),\n",
    "                \"page_no\": row[\"page_no\"],\n",
    "                \"classifications\": row[\"classifications\"],\n",
    "                \"text\": row.get(\"text\")\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        adage_data_model[\"events\"].append(event)\n",
    "    \n",
    "    adage_data_model[\"time_object\"][\"timestamp\"] = df[\"modified\"].max().isoformat()\n",
    "    \n",
    "    return json.dumps(adage_data_model, indent=4)\n",
    "\n",
    "def process_xml_file(file_path):\n",
    "    xml_data = open(file_path).read()\n",
    "    parser = etree.XMLParser(ns_clean=True)\n",
    "    xml = etree.parse(io.StringIO(xml_data), parser)\n",
    "    data = []\n",
    "\n",
    "    for dossier in xml.xpath('//dcdossier'):\n",
    "        guid = dossier.get('guid')\n",
    "        modified = dossier.get('modified')\n",
    "        \n",
    "        for doc in dossier.xpath('.//document'):\n",
    "            newspaper_code = doc.xpath('.//NEWSPAPERCODE/text()')\n",
    "            section = doc.xpath('.//SECTION/text()')\n",
    "            story_name = doc.xpath('.//STORYNAME/text()')\n",
    "            publication_date = doc.xpath('.//PUBLICATIONDATE/text()')\n",
    "            newspaper = doc.xpath('.//NEWSPAPER/text()')\n",
    "            page_no = doc.xpath('.//PAGENO/text()')\n",
    "            byline = doc.xpath('.//BYLINE/text()')\n",
    "            classifications = doc.xpath('.//CLASSIFICATION/text()')\n",
    "            headline = doc.xpath('.//HEADLINE/text()')\n",
    "            intro = doc.xpath('.//INTRO/text()')\n",
    "            text = \" \".join(doc.xpath('.//TEXT//text()'))\n",
    "            \n",
    "            data.append({\n",
    "                'guid': guid,\n",
    "                'modified': pd.to_datetime(modified, errors='coerce', utc=True),\n",
    "                'section': section[0].strip() if section else None,\n",
    "                'publication_date': pd.to_datetime(publication_date[0]) if publication_date else None,\n",
    "                'page_no': page_no[0].strip() if page_no else None,\n",
    "                'byline': byline[0].strip() if byline else None,\n",
    "                'classifications': classifications if classifications else None,\n",
    "                'headline': headline[0].strip() if headline else None,\n",
    "                'intro': intro[0].strip() if intro else None,\n",
    "                'text': text.strip() if text else None,\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "def process_all_files(directory):\n",
    "    all_dataframes = []\n",
    "    j = 0\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.xml'):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            df = process_xml_file(file_path)\n",
    "            all_dataframes.append(df)\n",
    "            print(f\"Processed data from {file_name}\")\n",
    "            j += 1\n",
    "        # if j == 10: \n",
    "        #     break\n",
    "    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "df = process_all_files('datasets/')\n",
    "# df = pd.DataFrame(data)\n",
    "# df['modified'] = pd.to_datetime(df['modified'])\n",
    "# df['publication_date'] = pd.to_datetime(df['publication_date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a json with only guid and author\n",
    "new_df = df[['guid', 'byline']].copy()\n",
    "\n",
    "# rename byline to author\n",
    "new_df.rename(columns={'byline': 'author'}, inplace=True)\n",
    "\n",
    "# Lowercase author field\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.lower() if x is not None else None)\n",
    "\n",
    "# First only take part of string before \" - \"\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split('-')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(';')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(',')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' & ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' and ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' with ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' | ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split('.')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' is ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' of ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' lord ')[0] if x is not None else None)\n",
    "new_df['author'] = new_df['author'].apply(lambda x: x.split(' by ')[0] if x is not None else None)\n",
    "# new_df['author'] = new_df['author'].apply(lambda x: re.findall(r'(?:(?<=^)|(?<=[^A-Za-z.,]))[A-Za-z.,]+(?: [A-Za-z.,]+)*(?:(?=[^A-Za-z.,])|(?=$))', x)[0] if x is not None else None)\n",
    "\n",
    "# Remove common phrases from author field and fix casing\n",
    "phrases = [\n",
    "    \"Afr Correspondent\",\n",
    "    \"Chief Political Correspondent\",\n",
    "    \"Political Correspondent\",\n",
    "    \"Political\",\n",
    "    \"Edited By\",\n",
    "    \"Editor\",\n",
    "    \"Story\",\n",
    "    \"Words\",\n",
    "    \"Political Editor\",\n",
    "    \"Edited By\",\n",
    "    \"Workplace Correspondent\",\n",
    "    \"National Affairs Correspondent\",\n",
    "    \"Economics Correspondent\",\n",
    "    \"Education\",\n",
    "    \"Executive\",\n",
    "    \"Director\",\n",
    "    \"Federal\",\n",
    "    \"Treasurer\",\n",
    "    \"Investment Strategy Ubs Asset Management\",\n",
    "    \"Workplace\",\n",
    "    \"Investor\",\n",
    "    \"Asia Pacific\",\n",
    "    \"Additional Reporting\",\n",
    "    \"Special\",\n",
    "    \"China\",\n",
    "    \"Correspondent\",\n",
    "    \"Property\",\n",
    "    \"East\",\n",
    "    \"Asia\",\n",
    "    \"Forum\",\n",
    "    \"Pensions\",\n",
    "    \"Pension\",\n",
    "    \"Limits\"\n",
    "]\n",
    "\n",
    "def fix_author(author):\n",
    "    if author is not None:\n",
    "        for phrase in phrases:\n",
    "            # author = author.lower().replace(phrase.lower(), '').strip().title()\n",
    "            # The above line was causing issues with the author names with the same phrase in the middle\n",
    "            author = re.sub(r'\\b' + re.escape(phrase) + r'\\b', '', author, flags=re.IGNORECASE).strip().title()\n",
    "            \n",
    "        return author\n",
    "    return None\n",
    "\n",
    "new_df['author'] = new_df['author'].apply(fix_author)\n",
    "\n",
    "# Set index\n",
    "new_df.set_index('guid', inplace=True)\n",
    "\n",
    "guids = None\n",
    "\n",
    "# Remove all guids not in guids.json\n",
    "with open('guids.json') as f:\n",
    "    guids = json.load(f)\n",
    "    \n",
    "new_df = new_df[new_df.index.isin(guids['guids'])]\n",
    "\n",
    "new_df.to_json('output.json', orient='index', indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Health/Death/Suicides',\n",
       " 'Labour/Harassment',\n",
       " 'Company/Downer Edi',\n",
       " 'Suicide',\n",
       " 'Cause of death',\n",
       " 'Psychological illnesses']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['classifications'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jeremy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Python(45968) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dissimilar labels among the most occurring labels:\n",
      "Landscape\n",
      "Professional achievement\n",
      "Economy/Monetary Policy\n",
      "Manager\n",
      "Stocks\n",
      "Investment bank\n",
      "Internet\n",
      "Energy carrier\n",
      "Economic policy\n",
      "Film industry\n",
      "Politics/Foreign Relations\n",
      "Investment\n",
      "Property/Residential Property/Units\n",
      "Holding\n",
      "Profit\n",
      "Election campaign\n",
      "Travel advice\n",
      "Credit\n",
      "Career planning\n",
      "Architecture\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# def split_labels(label_list):\n",
    "#     if label_list is None:\n",
    "#         return []\n",
    "#     split_labels = []\n",
    "#     for label in label_list:\n",
    "#         parts = label.split('/', maxsplit=1)\n",
    "#         split_labels.append(parts[0])\n",
    "#         if len(parts) > 1:\n",
    "#             split_labels.append(parts[1])\n",
    "#     return split_labels\n",
    "\n",
    "# all_labels = []\n",
    "# for labels in df['classifications']:\n",
    "#     all_labels.extend(split_labels(labels))\n",
    "\n",
    "# label_counts = Counter(all_labels)\n",
    "# top_labels = label_counts.most_common(100)\n",
    "\n",
    "# print(\"Top 100 labels:\")\n",
    "# for label, count in top_labels:\n",
    "#     print(f\"{label}: {count}\")\n",
    "# from sklearn.cluster import KMeans\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "# def train_word2vec(labels, min_count=1, vector_size=100):\n",
    "#     model = Word2Vec(labels, min_count=min_count, vector_size=vector_size)\n",
    "#     return model\n",
    "\n",
    "# def get_label_vectors(model, labels):\n",
    "#     label_vectors = []\n",
    "#     for label_parts in labels:\n",
    "#         vector = sum([model.wv[part] for part in label_parts if part in model.wv])\n",
    "#         label_vectors.append(vector)\n",
    "#     return label_vectors\n",
    "\n",
    "# def cluster_labels(label_vectors, n_clusters=10):\n",
    "#     kmeans = KMeans(n_clusters=n_clusters)\n",
    "#     kmeans.fit(label_vectors)\n",
    "#     return kmeans.labels_\n",
    "\n",
    "# all_labels = df['classifications'].dropna().tolist()\n",
    "\n",
    "# word2vec_model = train_word2vec(all_labels)\n",
    "\n",
    "# label_vectors = get_label_vectors(word2vec_model, all_labels)\n",
    "\n",
    "# n_clusters = 10\n",
    "# cluster_labels = cluster_labels(label_vectors, n_clusters)\n",
    "\n",
    "# label_clusters = {}\n",
    "# for label, cluster in zip(all_labels, cluster_labels):\n",
    "#     label_clusters['/'.join(label)] = cluster\n",
    "\n",
    "# for label, cluster in label_clusters.items():\n",
    "#     print(f\"{label}: Cluster {cluster}\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess(text):\n",
    "    \n",
    "    last_word = text.split('/')[-1]\n",
    "    tokens = word_tokenize(last_word)\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def select_representative_label(cluster_labels):\n",
    "    if not cluster_labels:\n",
    "        return None\n",
    "    \n",
    "    label_counts = Counter(cluster_labels)\n",
    "    representative_label = max(label_counts, key=label_counts.get)\n",
    "    \n",
    "    return representative_label\n",
    "\n",
    "all_labels = [label for labels in df['classifications'] if labels is not None for label in labels]\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "\n",
    "top_n = 100\n",
    "top_labels = [label for label, _ in label_counts.most_common(top_n)]\n",
    "\n",
    "preprocessed_labels = [preprocess(label) for label in top_labels]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "label_vectors = vectorizer.fit_transform(preprocessed_labels)\n",
    "\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(label_vectors)\n",
    "\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "representative_labels = []\n",
    "for cluster in range(n_clusters):\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "    cluster_labels_list = [top_labels[i] for i in cluster_indices]\n",
    "    \n",
    "    if cluster_labels_list:\n",
    "        representative_label = select_representative_label(cluster_labels_list)\n",
    "        representative_labels.append(representative_label)\n",
    "    else:\n",
    "        representative_labels.append(None)\n",
    "\n",
    "print(\"Dissimilar labels among the most occurring labels:\")\n",
    "for label in representative_labels:\n",
    "    if label is not None:\n",
    "        print(label)\n",
    "    else:\n",
    "        print(\"Empty Cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Ricardo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk_stopwords = stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_and_prepare_text(input_text, pattern=r'[^\\w\\s]', to_lowercase=True, filter_stopwords=True, stemming=True):\n",
    "    cleaned_text = re.sub(pattern, '', input_text)\n",
    "    if to_lowercase:\n",
    "        cleaned_text = cleaned_text.lower()\n",
    "    tokenized_text = word_tokenize(cleaned_text)\n",
    "    if filter_stopwords:\n",
    "        tokenized_text = [word for word in tokenized_text if word not in nltk_stopwords]\n",
    "    if stemming:\n",
    "        processor = PorterStemmer()\n",
    "    else:\n",
    "        processor = WordNetLemmatizer()\n",
    "    processed_text = [processor.stem(word) if stemming else processor.lemmatize(word) for word in tokenized_text]\n",
    "    return ' '.join(processed_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_settings = (r'[^\\w\\s]', False, True, False)\n",
    "pattern, lower, stopword_removal, stem = preprocess_settings\n",
    "preprocess_fn = lambda x: clean_and_prepare_text(x, pattern, lower, stopword_removal, stem)\n",
    "df['pre_processed_text'] = df['text'].apply(preprocess_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty_classifications_df = df[df['classifications'].apply(lambda x: len(x) == 0 if x is not None else True)]\n",
    "# len(empty_classifications_df)\n",
    "df['classifications'] = df['classifications'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "# df = df[df['classifications'].apply(lambda x: len(x) == 0)]\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_labels = mlb.fit_transform(df['classifications'])\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "# print(len(processed_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['pre_processed_text'], processed_labels, test_size=0.1, random_state=42)\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "multi_label_nb.fit(X_train_vec, y_train)\n",
    "y_pred = multi_label_nb.predict(X_test_vec)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Hamming Loss: {hamming_loss(y_test, y_pred):.4f}\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred, average='macro')\n",
    "recall = recall_score(y_test, y_pred, average='macro')\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "jaccard = jaccard_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Jaccard Score: {jaccard:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def update_dataframe_with_multi_label(df, preprocess_function, label_column='classifications'):\n",
    "#     updated_df = df.copy()\n",
    "#     updated_df['processed_text'] = updated_df['text'].apply(preprocess_function)\n",
    "    \n",
    "#     mlb = MultiLabelBinarizer()\n",
    "#     updated_df['processed_labels'] = list(mlb.fit_transform(updated_df[label_column]))\n",
    "    \n",
    "#     return updated_df, mlb.classes_\n",
    "\n",
    "\n",
    "def evaluate_multi_label_classification(preprocess_settings, df, label_column='classifications'):\n",
    "    global vectorizer, mlb, multi_label_nb\n",
    "    pattern, lower, stopword_removal, stem = preprocess_settings\n",
    "    print(f\"Configuration: Pattern={pattern}, Lowercase={lower}, Stemming={stem}\")\n",
    "\n",
    "    preprocess_fn = lambda x: clean_and_prepare_text(x, pattern, lower, stopword_removal, stem)\n",
    "    df['processed_text'] = df['text'].apply(preprocess_fn)\n",
    "\n",
    "    \n",
    "    df['classifications'] = df['classifications'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "    processed_labels = mlb.fit_transform(df['classifications'])\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['processed_text'], processed_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    multi_label_nb.fit(X_train_vec, y_train)\n",
    "\n",
    "\n",
    "    y_pred = multi_label_nb.predict(X_test_vec)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(f\"Hamming Loss: {hamming_loss(y_test, y_pred):.4f}\")\n",
    "\n",
    "    y_pred_inversed = mlb.inverse_transform(y_pred)\n",
    "    y_test_inversed = mlb.inverse_transform(y_test)\n",
    "\n",
    "    print(\"\\nSample of Actual vs. Predicted Classifications:\")\n",
    "    for i in range(10):\n",
    "        print(f\"Actual: {y_test_inversed[i]}, Predicted: {y_pred_inversed[i]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: Pattern=[^\\w\\s], Lowercase=False, Stemming=False\n",
      "Accuracy: 0.0228\n",
      "Hamming Loss: 0.0033\n",
      "\n",
      "Sample of Actual vs. Predicted Classifications:\n",
      "Actual: ('ECONOMY/COMMODITIES', 'Energy carrier', 'Export', 'Industry/Oil', 'Raw materials'), Predicted: ('Economic forecast', 'Raw materials')\n",
      "Actual: ('Laws', 'Presidential election', 'Right to vote'), Predicted: ('Election campaign', 'Presidential election')\n",
      "Actual: ('COMPANY/AMCOR LTD', 'Corporate structure', 'Fusion', 'Labour/Occupations/Management', 'Manager'), Predicted: ('Manager', 'Stock exchange', 'Stocks')\n",
      "Actual: ('Entrepreneurs', 'Fast climber', 'Labour/Occupations/Management', 'Manager'), Predicted: ()\n",
      "Actual: ('Economic policy', 'Economy/Finance', 'International economic relations', 'World economy'), Predicted: ()\n",
      "Actual: ('Heavy industry', 'Industry/Mining/Iron Ore', 'Raw materials', 'Stocks'), Predicted: ('Raw materials', 'Stock exchange', 'Stocks')\n",
      "Actual: ('Capital market', 'Company/AUSTRALIA & NEW ZEALAND BANKING GROUP LTD/ANZ', 'Credit', 'Economy/Investment/Personal', 'Investment counselling'), Predicted: ('Stock exchange',)\n",
      "Actual: ('Loss', 'Raw materials', 'Stocks'), Predicted: ('Raw materials', 'Stocks')\n",
      "Actual: ('Discrimination', 'Human right', 'Labour/Occupations/Legal profession', 'Law/Courts/Judges & Magistrates', 'Political commitment'), Predicted: ()\n",
      "Actual: ('Capital market', 'Economic forecast', 'World economy'), Predicted: ('Economic forecast',)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df['classifications'] = df['classifications'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "preprocess_settings = (r'[^\\w\\s]', False, True, False)\n",
    "evaluate_multi_label_classification(preprocess_settings, df, 'classifications')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended labels for your input: [('Achievement', 'Action group', 'Administrative law', 'Aristocrats', 'Business/Transaction Cards Credit Cards', 'COMPANY/AMCOR LTD', 'COMPANY/BOEING CO', 'COMPANY/CITIBANK LTD', 'COMPANY/MERITON PROPERTIES', 'Cattle breeding', 'Children', 'Civil law', 'Civil war', 'Company/Airbus Industries', 'Company/Amazon.com', 'Company/Ausnet', 'Company/Bradken Limited', 'Company/Charter Hall Holdings', 'Company/Coopers Brewery', 'Company/Corporate Travel Managemement Ltd', 'Company/John Fairfax Holdings/Fairfax Media Ltd', 'Company/Manulife Financial Corp', 'Company/Monadelphous Group Ltd', 'Company/NINE NETWORK AUSTRALIA LTD', 'Company/Orica', 'Company/PanAust Ltd', 'Company/Perpetual Investments', 'Company/Ramsay Health Care Ltd', 'Company/Royal Bank of Scotland', 'Company/Royal Dutch Shell/Shell', 'Company/Tesco Plc', 'Company/Transurban Group', 'Company/UBS Group', 'Company/Us Federal Reserve', 'Company/Wam Capital', 'Company/YouTube Inc', 'Credit card', 'Crime/Australians Overseas', 'Crime/Domestic Violence', 'Crime/Drugs', 'Crime/Organised Crime', 'Culture/Music', 'Economy/Taxation/Withholding Tax', 'Economy/Trade/Apec', 'Economy/Trade/Gatt & World Trade Organisation/WTO', 'Electronic banking', 'Financial policy', 'History', 'Income regulation', 'Industry/Exhibitions', 'LIFESTYLE/FURNITURE & OBJECTS/HOUSEHOLD/FURNITURE', 'Labour/Occupations/Authors & Playwrights', 'Labour/Occupations/Cleaners', 'Labour/Occupations/Consultants', 'Labour/Occupations/Health Workers/Pharmacists', 'Labour/Occupations/Public Servants', 'Law/Discrimination/Racial', 'Law/Wills', 'Lifestyle/Food & Drink/Cookery', 'Magazines', 'Media/Licenses/Licences', 'Media/Television/Videos/Recorders', 'POLITICS/POLITICAL PARTIES & GROUPS/GREENS', 'Payment transactions', 'People/Name/Andrews/Kevin/Politician', 'People/Name/Bernanke/Ben/Bureaucrat', 'People/Name/Jones/Alan/Radio Broadcaster', 'People/Name/Modi/Narendra/Politician', 'People/Name/Palmer/Clive/Business', 'People/Name/Sims/Rod/Business', 'People/People/Fans', 'Places & Locations/Coast/Beaches', 'Politics/Asean', 'Politics/Constitutions/Republicanism', 'Politics/Elections/By-elections', 'Politics/Political Parties & Groups/Democrats (USA)', 'Politics/Political Parties & Groups/Palmer United Party', 'Politics/Political Parties & Groups/Republicans (USA)', 'Politics/Terrorism/World Trade Center Pentagon/War', 'Population Groups/Aborigines', 'Population Groups/Families', 'Possession', 'Prison conditions', 'Property/Commercial Property/Shops', 'Property/Town Planning/East Darling Harbour Barangaroo', 'SOCIAL WELFARE/CHILD WELFARE', 'Social Welfare/Child Care', 'Sport/Gridiron', 'Sport/Match Fixing', 'State terror', 'Stock exchange', 'Stocks', 'TRANSPORT/ROAD TRANSPORT/TRUCKS', 'Tax evasion', 'Tax haven', 'Technology/Telecommunications/Satellites', 'Traffic accident', 'Transport/Aviation/Passengers', 'Transport/Tunnels', 'Vocational crime', 'War crime', 'Weather/Droughts', 'Weather/El Nino', 'Work time regulation', 'prominent man')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def recommend_labels_with_custom_threshold(input_text, threshold=0.001):\n",
    "    preprocessed_text = clean_and_prepare_text(input_text)\n",
    "    input_vec = vectorizer.transform([preprocessed_text])\n",
    "    predicted_probs = multi_label_nb.predict_proba(input_vec)\n",
    "    predictions = np.zeros((1, len(mlb.classes_)), dtype=int)\n",
    "    \n",
    "    for idx, probs in enumerate(predicted_probs):\n",
    "        if probs.shape[1] == 1:\n",
    "            predictions[0, idx] = probs[0, 0] >= threshold\n",
    "        else:\n",
    "            predictions[0, idx] = probs[0, 1] >= threshold\n",
    "    \n",
    "    recommended_labels = mlb.inverse_transform(predictions)\n",
    "    \n",
    "    return recommended_labels\n",
    "\n",
    "\n",
    "user_input = \"\"\"In today's rapidly evolving economic landscape, understanding the dynamics of the stock market has become more crucial than ever for investors and analysts alike. Amidst fluctuating market conditions, a recent trend has emerged, highlighting a significant shift towards technology and renewable energy sectors. This pivot is largely driven by global demands for sustainability and innovation, reshaping investment strategies across the board.\n",
    "The surge in tech and green energy investments is not merely a reaction to consumer preferences or environmental concerns; it's a strategic move by savvy investors aiming to capitalize on the future of global markets. Companies leading in renewable energy solutions and technological advancements are now at the forefront of stock market gains, outpacing traditional industries that once dominated the financial landscape.\n",
    "Moreover, this shift is also influencing policy decisions and corporate strategies, with a growing emphasis on ESG (Environmental, Social, and Governance) criteria. Investors are increasingly factoring in companies' ESG scores when making investment decisions, recognizing that sustainable practices are key to long-term profitability and risk management.\n",
    "As we navigate through these changing tides, the stock market's landscape continues to evolve, presenting new opportunities and challenges for investors. Staying informed and adaptable is paramount, as the sectors that are leading today may pave the way for the economic paradigms of tomorrow. In essence, the current trends underscore a broader movement towards a more sustainable and technologically driven global economy, heralding a new era in financial investment.\n",
    "\"\"\"\n",
    "recommended_labels = recommend_labels_with_custom_threshold(user_input)\n",
    "print(f\"Recommended labels for your input: {recommended_labels}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
